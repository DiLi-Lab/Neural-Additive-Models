{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: A100-SXM4-40GB, pci bus id: 0000:0f:00.0, compute capability: 8.0\n",
      "\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/apps/Anaconda3/2020.11/lib/python3.8/site-packages/IPython/core/magics/pylab.py:159: UserWarning: pylab import has clobbered these variables: ['random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/prasse/work/Projekte/AEye/reading-comprehension/')\n",
    "import os\n",
    "import tensorflow as tf\n",
    "GPU = 1\n",
    "# select graphic card\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "config = tf.compat.v1.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "config.gpu_options.allow_growth = True\n",
    "tf_session = tf.compat.v1.Session(config=config)\n",
    "from rf_baseline  import rf_feature_extraction as rf_feature_extraction\n",
    "import datetime\n",
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import json\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow import Variable\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.math import l2_normalize\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import AveragePooling1D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPool1D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.models import clone_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Embedding, Average, Dropout\n",
    "\n",
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d0f2a2b8aab5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpos_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc_level'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mneg_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc_level'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpos_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mx_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train_all' is not defined"
     ]
    }
   ],
   "source": [
    "pos_ids = np.where(y_train_all[:,label_dict['acc_level']] == 1)[0]\n",
    "neg_ids = np.where(y_train_all[:,label_dict['acc_level']] == 0)[0]\n",
    "for pos_id in pos_ids:\n",
    "    x_instance = x_train_all[pos_id,:,-2]\n",
    "    y_instance = x_train_all[pos_id,:,-1]\n",
    "    use_ids = np.logical_and(x_instance != 0,\n",
    "                            y_instance != 0)\n",
    "    x_mean = np.mean(x_instance[use_ids])\n",
    "    y_mean = np.mean(y_instance[use_ids])\n",
    "    plt.plot(x_mean,y_mean,'rx')\n",
    "\n",
    "for neg_id in neg_ids:\n",
    "    x_instance = x_train_all[neg_id,:,-2]\n",
    "    y_instance = x_train_all[neg_id,:,-1]\n",
    "    use_ids = np.logical_and(x_instance != 0,\n",
    "                            y_instance != 0)\n",
    "    x_mean = np.mean(x_instance[use_ids])\n",
    "    y_mean = np.mean(y_instance[use_ids])\n",
    "    plt.plot(x_mean,y_mean,'bx')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_model(ent_len,num_ent,\n",
    "                 pos_len,num_pos,\n",
    "                 num_features,input_len_fix=398,num_fix=4):\n",
    "    \n",
    "    drop_outs = 0.5\n",
    "    \n",
    "    x_ent_input = Input(shape=ent_len)\n",
    "    x_ent = Embedding(input_dim = num_ent,\n",
    "                        output_dim = 64)(x_ent_input)\n",
    "    x_ent_avg = GlobalAveragePooling1D()(x_ent)\n",
    "    x_ent_max = GlobalMaxPool1D()(x_ent)\n",
    "    x_ent = Concatenate()([x_ent_avg,x_ent_max])\n",
    "    x_ent = Dropout(0.2)(x_ent)\n",
    "    x_ent = Dense(64,activation='relu')(x_ent)\n",
    "    x_ent = Dense(32,activation='relu')(x_ent)\n",
    "\n",
    "\n",
    "    x_pos_input = Input(shape=pos_len)\n",
    "    x_pos = Embedding(input_dim = num_pos,\n",
    "                        output_dim = 128)(x_pos_input)\n",
    "    x_pos_avg = GlobalAveragePooling1D()(x_pos)\n",
    "    x_pos_max = GlobalMaxPool1D()(x_pos)\n",
    "    x_pos = Concatenate()([x_pos_avg,x_pos_max])\n",
    "    x_pos = Dropout(0.2)(x_pos)\n",
    "    x_pos = Dense(128,activation='relu')(x_pos)\n",
    "    x_pos = Dense(64,activation='relu')(x_pos)\n",
    "    x_pos = Dense(32,activation='relu')(x_pos)\n",
    "\n",
    "\n",
    "    x_num_input = Input(shape=num_features)\n",
    "    x_num = Dense(256,activation='relu')(x_num_input)\n",
    "    x_num = Dropout(0.2)(x_num)\n",
    "    x_num = Dense(128,activation='relu')(x_num)\n",
    "    x_num = Dropout(drop_outs)(x_num)\n",
    "    x_num = Dense(64,activation='relu')(x_num)\n",
    "    x_num = Dropout(drop_outs)(x_num)\n",
    "    x_num = Dense(32,activation='relu')(x_num)\n",
    "    \n",
    "    x_fix_input = Input(shape=(input_len_fix, num_fix))\n",
    "    x_fix = Conv1D(64, 5, activation='gelu')(x_fix_input)\n",
    "    x_fix = AveragePooling1D(pool_size=2)(x_fix)\n",
    "    x_fix = BatchNormalization()(x_fix)\n",
    "    x_fix_avg = GlobalAveragePooling1D()(x_fix)\n",
    "    x_fix_max = GlobalMaxPool1D()(x_fix)\n",
    "    x_fix = Concatenate()([x_fix_avg,x_fix_max])\n",
    "    x_fix = Dropout(drop_outs)(x_fix)\n",
    "    x_fix = Dense(32,activation='relu')(x_fix)\n",
    "\n",
    "    x_concat = Concatenate()([x_ent,x_pos,x_num,x_fix])\n",
    "    x_output = Dense(32,activation='relu')(x_concat)\n",
    "    x_output = Dense(1,activation='sigmoid')(x_output)\n",
    "    model = Model(inputs=[\n",
    "                x_ent_input,\n",
    "                x_pos_input,\n",
    "                x_num_input,\n",
    "                x_fix_input],\n",
    "                outputs=x_output)\n",
    "    opt = Adam()\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', auroc])\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help_roc_auc(y_true,y_pred):\n",
    "    if len(np.unique(y_true)) == 1:\n",
    "        return .5\n",
    "    else:\n",
    "        return roc_auc_score(y_true,y_pred)\n",
    "\n",
    "# calculate the roc-auc as a metric\n",
    "def auroc(y_true, y_pred):\n",
    "    return tf.py_function(help_roc_auc, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_criterion = 'subj'\n",
    "#split_criterion = 'book-page'\n",
    "#split_criterion = 'book'\n",
    "label = 'acc_level'\n",
    "#label = 'native'\n",
    "#label = 'subj_acc_level'\n",
    "#label = 'difficulty'\n",
    "normalize_flag = True\n",
    "grid_search = False\n",
    "use_gaze_entropy_features = True\n",
    "input_window = 'page'\n",
    "num_words = 150\n",
    "\n",
    "patience = 50\n",
    "batch_size = 256\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/raid/projects/reich3/reading-comprehension/paper_splits/subj/X_train_subj_text_sequence_with_paul_david_cluster_41_feature_0.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6a2952d4fd52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mX_train_fix_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSB_SAT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'X_train_{split_criterion}{feature_string}{fold}_fix_data.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0my_train_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSB_SAT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'y_train_{split_criterion}{feature_string}{fold}.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mx_train_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mx_train_fix_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_fix_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnormalize_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/apps/Anaconda3/2020.11/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/raid/projects/reich3/reading-comprehension/paper_splits/subj/X_train_subj_text_sequence_with_paul_david_cluster_41_feature_0.npy'"
     ]
    }
   ],
   "source": [
    "spit_criterions = ['subj','book-page','book']\n",
    "labels = ['acc_level','native','difficulty','subj_acc_level']\n",
    "\n",
    "model_name = 'nn_paul'\n",
    "axis = 1\n",
    "n_estimators = 100\n",
    "\n",
    "feature_names_per_word = [      'ff',\n",
    "                                'tf',\n",
    "                                'wfc_ff_nomarlized',\n",
    "                                'wfc_ff_normalized',\n",
    "                                'sc_ff_normalized',\n",
    "                                'sc_tf_normalized',\n",
    "                                'ic_ff_normalized',\n",
    "                                'regression',\n",
    "                                'num_regressions',\n",
    "                                'num_progressions',\n",
    "                                'surprisal',\n",
    "                                'word_len']\n",
    "    \n",
    "\n",
    "for entity in rf_feature_extraction.entity_list:\n",
    "    feature_names_per_word.append('is_entity_' + entity)\n",
    "\n",
    "for pos in rf_feature_extraction.pos_list:\n",
    "    feature_names_per_word.append('is_pos_' + pos)\n",
    "\n",
    "feature_names_per_word += ['x_mean','y_mean']\n",
    "\n",
    "feature_string = '_text_sequence_'\n",
    "feature_string = '_text_sequence_with_paul_david_cluster_'\n",
    "feature_string = '_text_sequence_with_paul_david_cluster_41_feature_'\n",
    "\n",
    "for split_criterion in spit_criterions:\n",
    "    for label in labels:\n",
    "        save_path=f'/home/reich3/tmp/results/reading-comprehension/{model_name}_{split_criterion}_text_sequence_{label}_{input_window}_1d_conv.csv'\n",
    "\n",
    "        SB_SAT_PATH = f'/raid/projects/reich3/reading-comprehension/paper_splits/{split_criterion}/'\n",
    "        split_criterion_dict = {\n",
    "            'subj': 0,\n",
    "            'book': 1,\n",
    "        }\n",
    "        with open('/raid/projects/reich3/reading-comprehension/paper_splits/labels_dict.json') as fp:\n",
    "            label_dict = json.load(fp)\n",
    "\n",
    "        if split_criterion == 'book':\n",
    "            num_folds = 4\n",
    "        else:\n",
    "            num_folds = 5\n",
    "        pd_init = pd.DataFrame(\n",
    "            columns=[\n",
    "                'ahn_baseline',\n",
    "                'fold0_auc', 'fold1_auc', 'fold2_auc', 'fold3_auc', 'fold4_auc',\n",
    "                'fold0_tpr', 'fold1_tpr', 'fold2_tpr', 'fold3_tpr', 'fold4_tpr',\n",
    "                'fold0_fpr', 'fold1_fpr', 'fold2_fpr', 'fold3_fpr', 'fold4_fpr',\n",
    "                'fold0_y_pred', 'fold1_y_pred', 'fold2_y_pred', 'fold3_y_pred', 'fold4_y_pred',\n",
    "                'fold0_y_test', 'fold1_y_test', 'fold2_y_test', 'fold3_y_test', 'fold4_y_test',\n",
    "            'avg_auc', 'std_auc']\n",
    "        )\n",
    "        pd_init['ahn_baseline'] = [model_name]\n",
    "\n",
    "        if input_window == 'window':\n",
    "            input_shape = (21, )\n",
    "        elif input_window == 'page':\n",
    "            input_shape = (150, 11)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        feature_importances = []\n",
    "        for fold in range(num_folds):\n",
    "            np.random.seed(fold)\n",
    "            random.seed(fold)\n",
    "            X_train_path = os.path.join(SB_SAT_PATH, f'X_train_{split_criterion}{feature_string}{fold}.npy')\n",
    "            X_train_fix_path = os.path.join(SB_SAT_PATH, f'X_train_{split_criterion}{feature_string}{fold}_fix_data.npy')\n",
    "            y_train_path = os.path.join(SB_SAT_PATH, f'y_train_{split_criterion}{feature_string}{fold}.npy')\n",
    "            x_train_all, y_train_all = np.load(X_train_path), np.load(y_train_path, allow_pickle=True)\n",
    "            x_train_fix_all = np.load(X_train_fix_path)\n",
    "            if normalize_flag:\n",
    "                scaler = MinMaxScaler()\n",
    "                fix_scaler = MinMaxScaler()\n",
    "                x_train_all = scaler.fit_transform(x_train_all.reshape(-1, x_train_all.shape[-1])).reshape(x_train_all.shape)\n",
    "                x_train_fix_all = fix_scaler.fit_transform(x_train_fix_all.reshape(-1, x_train_fix_all.shape[-1])).reshape(x_train_fix_all.shape)\n",
    "            \n",
    "            if split_criterion != 'book':\n",
    "                outer_cv = KFold(n_splits=4, shuffle=True, random_state=fold)\n",
    "            else:\n",
    "                outer_cv = KFold(n_splits=3, shuffle=True, random_state=fold)\n",
    "\n",
    "            if split_criterion != 'book-page':\n",
    "                splitkeys = np.array(sorted(list(set(y_train_all[:, split_criterion_dict[split_criterion]]))))\n",
    "            else:\n",
    "                splitkeys = y_train_all[:, label_dict[label]]\n",
    "\n",
    "            for train_idx, val_idx in outer_cv.split(splitkeys):\n",
    "                break\n",
    "\n",
    "            if split_criterion != 'book-page':\n",
    "                N_train_sub = splitkeys[train_idx]\n",
    "                N_test_sub = splitkeys[val_idx]\n",
    "                \n",
    "                train_idx = np.where(np.isin(y_train_all[:, split_criterion_dict[split_criterion]], N_train_sub))[0]\n",
    "                val_idx = np.where(np.isin(y_train_all[:, split_criterion_dict[split_criterion]], N_test_sub))[0]\n",
    "            x_train = x_train_all[train_idx]\n",
    "            x_train_fix = x_train_fix_all[train_idx]\n",
    "            y_train = y_train_all[train_idx]\n",
    "            x_val = x_train_all[val_idx]\n",
    "            x_val_fix = x_train_fix_all[val_idx]\n",
    "            y_val = y_train_all[val_idx] \n",
    "            \n",
    "            y_train = np.array(y_train[:, label_dict[label]], dtype=int)\n",
    "            y_val = np.array(y_val[:, label_dict[label]], dtype=int)\n",
    "\n",
    "            x_train, feature_names = rf_feature_extraction.get_rf_features_for_word_features(\n",
    "                        x_train,feature_names_per_word,disable = False,\n",
    "                        use_gaze_entropy_features = use_gaze_entropy_features)\n",
    "            x_val, _ = rf_feature_extraction.get_rf_features_for_word_features(\n",
    "                        x_val,feature_names_per_word,disable = False,\n",
    "                        use_gaze_entropy_features = use_gaze_entropy_features)\n",
    "            \n",
    "            \n",
    "            # Test Data\n",
    "            X_test_path = os.path.join(SB_SAT_PATH, f'X_test_{split_criterion}{feature_string}{fold}.npy')\n",
    "            y_test_path = os.path.join(SB_SAT_PATH, f'y_test_{split_criterion}{feature_string}{fold}.npy')\n",
    "            X_test_fix_path = os.path.join(SB_SAT_PATH, f'X_test_{split_criterion}{feature_string}{fold}_fix_data.npy')\n",
    "            x_test_all, y_test_all = np.load(X_test_path), np.load(y_test_path, allow_pickle=True)\n",
    "            x_test_fix_all = np.load(X_test_fix_path)\n",
    "            if normalize_flag:\n",
    "                x_test_all = scaler.transform(x_test_all.reshape(-1, x_test_all.shape[-1])).reshape(x_test_all.shape)\n",
    "                x_test_fix_all = fix_scaler.transform(x_test_fix_all.reshape(-1, x_test_fix_all.shape[-1])).reshape(x_test_fix_all.shape)\n",
    "            \n",
    "            y_test = np.array(y_test_all[:, label_dict[label]], dtype=int)\n",
    "\n",
    "\n",
    "            x_test, _ = rf_feature_extraction.get_rf_features_for_word_features(\n",
    "                        x_test_all,feature_names_per_word,disable = False,\n",
    "                        use_gaze_entropy_features = use_gaze_entropy_features)\n",
    "            \n",
    "            \n",
    "            # POS + Entity Lists\n",
    "            pos_list_list_train, entity_list_list_train, pos_feature_names, entity_feature_names = rf_feature_extraction.get_watched_pos_entity_lists(\n",
    "                    x_train_all,feature_names_per_word)\n",
    "            pos_list_list_test, entity_list_list_test, _, _ = rf_feature_extraction.get_watched_pos_entity_lists(\n",
    "                    x_test_all,feature_names_per_word)\n",
    "            \n",
    "            pos_lens     = [len(pos_list) for pos_list in pos_list_list_train]\n",
    "            entity_lens  = [len(ent_list) for ent_list in entity_list_list_train]\n",
    "            max_pos_len  = np.max(pos_lens)\n",
    "            max_ent_len  = np.max(entity_lens)\n",
    "            \n",
    "            train_pos_matrix, pos_mapping_dict = rf_feature_extraction.convert_to_sequence_to_nn_input(\n",
    "                            pos_list_list_train,pos_feature_names,max_pos_len)\n",
    "            test_pos_matrix, _ = rf_feature_extraction.convert_to_sequence_to_nn_input(\n",
    "                            pos_list_list_test,pos_feature_names,max_pos_len,pos_mapping_dict)\n",
    "            train_ent_matrix, ent_mapping_dict = rf_feature_extraction.convert_to_sequence_to_nn_input(\n",
    "                            entity_list_list_train,entity_feature_names,max_ent_len)\n",
    "            test_ent_matrix, _ = rf_feature_extraction.convert_to_sequence_to_nn_input(\n",
    "                            entity_list_list_test,entity_feature_names,max_ent_len,ent_mapping_dict)\n",
    "            \n",
    "            \n",
    "            num_pos      = len(pos_mapping_dict) + 1\n",
    "            num_ent      = len(ent_mapping_dict) + 1\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            val_pos_matrix   = train_pos_matrix[val_idx]\n",
    "            train_pos_matrix = train_pos_matrix[train_idx]\n",
    "            test_pos_matrix  = test_pos_matrix\n",
    "            \n",
    "            val_ent_matrix   = train_ent_matrix[val_idx]\n",
    "            train_ent_matrix = train_ent_matrix[train_idx]\n",
    "            test_ent_matrix  = test_ent_matrix\n",
    "            \n",
    "            \n",
    "            ent_len = train_ent_matrix.shape[1]\n",
    "            pos_len = train_pos_matrix.shape[1]\n",
    "            num_features = x_train.shape[1]\n",
    "            model = get_nn_model(ent_len,num_ent,\n",
    "                                 pos_len,num_pos,\n",
    "                                 num_features)\n",
    "            \n",
    "            \n",
    "            # scale the RF input\n",
    "            rf_input_scaler = MinMaxScaler()\n",
    "            x_train = rf_input_scaler.fit_transform(x_train)\n",
    "            x_val = rf_input_scaler.transform(x_val)\n",
    "            x_test = rf_input_scaler.transform(x_test)\n",
    "            \n",
    "            tf.keras.backend.clear_session()\n",
    "            callbacks = [EarlyStopping(monitor='val_loss', patience=patience)]\n",
    "            history = model.fit([train_ent_matrix,\n",
    "                                 train_pos_matrix,\n",
    "                                 x_train,x_train_fix],y_train,\n",
    "                                         validation_data = (\n",
    "                                             [val_ent_matrix,\n",
    "                                              val_pos_matrix,\n",
    "                                              x_val,x_val_fix],\n",
    "                                             y_val),\n",
    "                                         batch_size = batch_size,\n",
    "                                         epochs = epochs,\n",
    "                                         callbacks = callbacks,\n",
    "                                         verbose = 0)\n",
    "            \n",
    "\n",
    "            y_pred = model.predict([test_ent_matrix,\n",
    "                                      test_pos_matrix,\n",
    "                                      x_test,x_test_fix_all],\n",
    "                                   batch_size = batch_size)\n",
    "            try:\n",
    "                fpr, tpr, _ = metrics.roc_curve(\n",
    "                    y_test,\n",
    "                    y_pred,\n",
    "                    pos_label=1,\n",
    "                )\n",
    "                auc = metrics.auc(fpr, tpr)\n",
    "                print(split_criterion, label, auc)\n",
    "                pd_init[f'fold{fold}_auc'] = auc\n",
    "                pd_init[f'fold{fold}_tpr'] = [tpr]\n",
    "                pd_init[f'fold{fold}_fpr'] = [fpr]\n",
    "                pd_init[f'fold{fold}_y_test'] = [y_test]\n",
    "                pd_init[f'fold{fold}_y_pred'] = [y_pred]\n",
    "            except:\n",
    "                try:\n",
    "                    fpr, tpr, _ = metrics.roc_curve(\n",
    "                        y_test,\n",
    "                        y_pred,\n",
    "                        pos_label=1,\n",
    "                    )\n",
    "                    auc = metrics.auc(fpr, tpr)\n",
    "                    print(split_criterion, label, auc)\n",
    "                    pd_init[f'fold{fold}_auc'] = auc\n",
    "                    pd_init[f'fold{fold}_tpr'] = [tpr]\n",
    "                    pd_init[f'fold{fold}_fpr'] = [fpr]\n",
    "                    pd_init[f'fold{fold}_y_test'] = y_test\n",
    "                    pd_init[f'fold{fold}_y_pred'] = y_pred\n",
    "                except:\n",
    "                    print(allo)\n",
    "\n",
    "        pd_init['avg_auc'] = 0\n",
    "        for i in range(num_folds):\n",
    "            pd_init['avg_auc'] += pd_init[f'fold{i}_auc']\n",
    "        pd_init['avg_auc']  /= num_folds\n",
    "\n",
    "        pd_init['std_auc'] = 0\n",
    "        for i in range(0, num_folds):\n",
    "            pd_init['std_auc'] +=  (pd_init[f'fold{i}_auc'] - pd_init['avg_auc'])**2\n",
    "        pd_init['std_auc'] = (pd_init['std_auc']/num_folds)**(1/2)\n",
    "        pd_init.to_csv(save_path, index=None)\n",
    "        print(f\"mean auc: {pd_init['avg_auc'].values[0]:.6f}\\pm{pd_init['std_auc'].values[0]:.6f}\")\n",
    "        #print(allo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_init.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ent_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_nn_model(ent_len,pos_len,num_features,num_pos,num_features)\n",
    "            \n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=patience)]\n",
    "\n",
    "history_val = model.fit([train_ent_matrix,\n",
    "                     train_pos_matrix,\n",
    "                     x_train],y_train,\n",
    "                             validation_data = (\n",
    "                                 [val_ent_matrix,\n",
    "                                  val_pos_matrix,\n",
    "                                  x_val],\n",
    "                                 y_val),\n",
    "                             batch_size = batch_size,\n",
    "                             epochs = epochs,\n",
    "                             callbacks = callbacks,\n",
    "                             verbose = 1)\n",
    "\n",
    "model = get_nn_model(ent_len,pos_len,num_features)\n",
    "\n",
    "history_test = model.fit([train_ent_matrix,\n",
    "                     train_pos_matrix,\n",
    "                     x_train],y_train,\n",
    "                             validation_data = (\n",
    "                                 [test_ent_matrix,\n",
    "                                  test_pos_matrix,\n",
    "                                  x_test],\n",
    "                                 y_test),\n",
    "                             batch_size = batch_size,\n",
    "                             epochs = epochs,\n",
    "                             callbacks = callbacks,\n",
    "                             verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(history_val.history['val_auroc'])),history_val.history['val_auroc'],label='validation')\n",
    "plt.plot(np.arange(len(history_val.history['val_auroc'])),history_val.history['auroc'],label='train')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(history_test.history['val_auroc'])),history_test.history['val_auroc'],label='validation')\n",
    "plt.plot(np.arange(len(history_test.history['val_auroc'])),history_test.history['auroc'],label='train')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_test.history['val_auroc'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([test_ent_matrix,\n",
    "                          test_pos_matrix,\n",
    "                          x_test],\n",
    "                       batch_size = batch_size)\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    pos_label=1,\n",
    ")\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_len = train_ent_matrix.shape[1]\n",
    "pos_len = train_pos_matrix.shape[1]\n",
    "num_features = x_train.shape[1]\n",
    "model = get_nn_model(ent_len,pos_len,num_features,num_pos,num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(history.history['val_auroc'])),history.history['val_auroc'],label='validation')\n",
    "plt.plot(np.arange(len(history.history['val_auroc'])),history.history['auroc'],label='train')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X Y input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_inputs = x_train_all[:,:,-2]\n",
    "#y_train_inputs = x_train_all[:,:,-1]\n",
    "train_inputs   = x_train_all[:,:,[-2,-1]]\n",
    "val_inputs     = train_inputs[val_idx]\n",
    "train_inputs   = train_inputs[train_idx]\n",
    "#x_test_inputs  = x_test_all[:,:,-2]\n",
    "#y_test_inputs  = x_test_all[:,:,-2]\n",
    "test_inputs    = x_test_all[:,:,[-2,-1]]\n",
    "\n",
    "x_input = Input(shape=(train_inputs.shape[1],train_inputs.shape[2]))\n",
    "x_avg = GlobalAveragePooling1D()(x_input)\n",
    "x_max = GlobalMaxPool1D()(x_input)\n",
    "x_output = Concatenate()([x_avg,x_max])\n",
    "#x_output = Dense(256,activation='relu')(x_output)\n",
    "#x_output = Dense(128,activation='relu')(x_output)\n",
    "#x_output = Dropout(0.2)(x_output)\n",
    "#x_output = Dense(64,activation='relu')(x_output)\n",
    "#x_output = Dense(32,activation='relu')(x_output)\n",
    "#x_output = Dense(16,activation='relu')(x_output)\n",
    "x_output = Dense(1,activation='sigmoid')(x_output)\n",
    "model = Model(inputs=x_input, outputs=x_output)\n",
    "opt = Adam()\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', auroc])\n",
    "model.summary()\n",
    "\n",
    "patience = 50\n",
    "batch_size = 256\n",
    "epochs = 1000\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_auroc', patience=patience)]\n",
    "history = model.fit(train_inputs,y_train,\n",
    "                             validation_data = (val_inputs,y_val),\n",
    "                             batch_size = batch_size,\n",
    "                             epochs = epochs,\n",
    "                             callbacks = callbacks,\n",
    "                             verbose = 0)\n",
    "\n",
    "plt.plot(np.arange(len(history.history['val_auroc'])),history.history['val_auroc'],label='validation')\n",
    "plt.plot(np.arange(len(history.history['val_auroc'])),history.history['auroc'],label='train')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(history.history['val_auroc'][-1])\n",
    "\n",
    "y_pred = model.predict(test_inputs,\n",
    "                       batch_size = batch_size)\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    pos_label=1,\n",
    ")\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_inputs = x_train_all[:,:,-2]\n",
    "#y_train_inputs = x_train_all[:,:,-1]\n",
    "train_inputs   = x_train_all[:,:,[-2,-1]]\n",
    "val_inputs     = train_inputs[val_idx]\n",
    "train_inputs   = train_inputs[train_idx]\n",
    "#x_test_inputs  = x_test_all[:,:,-2]\n",
    "#y_test_inputs  = x_test_all[:,:,-2]\n",
    "test_inputs    = x_test_all[:,:,[-2,-1]]\n",
    "\n",
    "x_input = Input(shape=(train_inputs.shape[1],train_inputs.shape[2]))\n",
    "x = Conv1D(32, 4, activation='gelu')(x_input)\n",
    "x = AveragePooling1D(pool_size=2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv1D(64, 2, activation='gelu')(x)\n",
    "x = AveragePooling1D(pool_size=2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "#x = Conv1D(64, 4, activation='gelu')(x)\n",
    "#x = AveragePooling1D(pool_size=2)(x)\n",
    "#x = BatchNormalization()(x)\n",
    "#x = Flatten()(x)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "#x = Dense(1024,activation='relu')(x)\n",
    "#x = Dropout(0.5)(x)\n",
    "#x = Dense(512,activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(32,activation='relu')(x)\n",
    "x_output = Dense(1,activation='sigmoid')(x)\n",
    "model = Model(inputs=x_input, outputs=x_output)\n",
    "opt = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', auroc])\n",
    "model.summary()\n",
    "\n",
    "patience = 5000\n",
    "batch_size = 256\n",
    "epochs = 1000\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_auroc', patience=patience)]\n",
    "history = model.fit(train_inputs,y_train,\n",
    "                             validation_data = (val_inputs,y_val),\n",
    "                             batch_size = batch_size,\n",
    "                             epochs = epochs,\n",
    "                             callbacks = callbacks,\n",
    "                             verbose = 0)\n",
    "\n",
    "plt.plot(np.arange(len(history.history['val_auroc'])),history.history['val_auroc'],label='validation')\n",
    "plt.plot(np.arange(len(history.history['val_auroc'])),history.history['auroc'],label='train')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(history.history['val_auroc'][-1])\n",
    "\n",
    "y_pred = model.predict(test_inputs,\n",
    "                       batch_size = batch_size)\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    pos_label=1,\n",
    ")\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_input = Input(shape=train_pos_matrix.shape[1])\n",
    "x_output = Embedding(input_dim = num_pos,\n",
    "                    output_dim = 128)(x_input)\n",
    "x_avg = GlobalAveragePooling1D()(x_output)\n",
    "x_max = GlobalMaxPool1D()(x_output)\n",
    "x_output = Concatenate()([x_avg,x_max])\n",
    "#x_output = Dense(256,activation='relu')(x_output)\n",
    "x_output = Dense(128,activation='relu')(x_output)\n",
    "#x_output = Dropout(0.2)(x_output)\n",
    "x_output = Dense(64,activation='relu')(x_output)\n",
    "x_output = Dense(32,activation='relu')(x_output)\n",
    "x_output = Dense(16,activation='relu')(x_output)\n",
    "x_output = Dense(1,activation='sigmoid')(x_output)\n",
    "model = Model(inputs=x_input, outputs=x_output)\n",
    "opt = Adam()\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', auroc])\n",
    "model.summary()\n",
    "\n",
    "patience = 50\n",
    "batch_size = 256\n",
    "epochs = 1000\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_auroc', patience=patience)]\n",
    "history = model.fit(train_pos_matrix,y_train,\n",
    "                             validation_data = (val_pos_matrix,y_val),\n",
    "                             batch_size = batch_size,\n",
    "                             epochs = epochs,\n",
    "                             callbacks = callbacks,\n",
    "                             verbose = 0)\n",
    "\n",
    "plt.plot(np.arange(len(history.history['val_auroc'])),history.history['val_auroc'],label='validation')\n",
    "plt.plot(np.arange(len(history.history['val_auroc'])),history.history['auroc'],label='train')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(history.history['val_auroc'][-1])\n",
    "\n",
    "y_pred = model.predict(test_pos_matrix,\n",
    "                       batch_size = batch_size)\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    pos_label=1,\n",
    ")\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS 1DCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = Input(shape=train_pos_matrix.shape[1])\n",
    "x_emb = Embedding(input_dim = num_pos,\n",
    "                    output_dim = 32)(x_input)\n",
    "x = Conv1D(64, 3, activation='gelu')(x_emb)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Flatten()(x)\n",
    "x_avg = GlobalAveragePooling1D()(x_emb)\n",
    "concat = Concatenate()([x,x_avg])\n",
    "x_output = Dense(32,activation='relu')(concat)\n",
    "x_output = Dense(1,activation='sigmoid')(x_output)\n",
    "model = Model(inputs=x_input, outputs=x_output)\n",
    "opt = Adam()\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', auroc])\n",
    "model.summary()\n",
    "\n",
    "patience = 50\n",
    "batch_size = 256\n",
    "epochs = 1000\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_auroc', patience=patience)]\n",
    "history = model.fit(train_pos_matrix,y_train,\n",
    "                             validation_data = (val_pos_matrix,y_val),\n",
    "                             batch_size = batch_size,\n",
    "                             epochs = epochs,\n",
    "                             callbacks = callbacks,\n",
    "                             verbose = 0)\n",
    "\n",
    "plt.plot(np.arange(len(history.history['val_auroc'])),history.history['val_auroc'],label='validation')\n",
    "plt.plot(np.arange(len(history.history['val_auroc'])),history.history['auroc'],label='train')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(history.history['val_auroc'][-1])\n",
    "\n",
    "y_pred = model.predict(test_pos_matrix,\n",
    "                       batch_size = batch_size)\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    pos_label=1,\n",
    ")\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = Input(shape=train_ent_matrix.shape[1])\n",
    "x_output = Embedding(input_dim = num_ent,\n",
    "                    output_dim = 64)(x_input)\n",
    "x_avg = GlobalAveragePooling1D()(x_output)\n",
    "x_max = GlobalMaxPool1D()(x_output)\n",
    "x_output = Concatenate()([x_avg,x_max])\n",
    "#x_output = Dense(256,activation='relu')(x_output)\n",
    "#x_output = Dense(128,activation='relu')(x_output)\n",
    "x_output = Dense(64,activation='relu')(x_output)\n",
    "x_output = Dense(32,activation='relu')(x_output)\n",
    "x_output = Dense(16,activation='relu')(x_output)\n",
    "x_output = Dense(1,activation='sigmoid')(x_output)\n",
    "model = Model(inputs=x_input, outputs=x_output)\n",
    "opt = Adam()\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', auroc])\n",
    "model.summary()\n",
    "\n",
    "patience = 50\n",
    "batch_size = 256\n",
    "epochs = 1000\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_auroc', patience=patience)]\n",
    "history = model.fit(train_ent_matrix,y_train,\n",
    "                             validation_data = (val_ent_matrix,y_val),\n",
    "                             batch_size = batch_size,\n",
    "                             epochs = epochs,\n",
    "                             callbacks = callbacks,\n",
    "                             verbose = 0)\n",
    "\n",
    "plt.plot(np.arange(len(history.history['val_auroc'])),history.history['val_auroc'],label='validation')\n",
    "plt.plot(np.arange(len(history.history['val_auroc'])),history.history['auroc'],label='train')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(history.history['val_auroc'][-1])\n",
    "\n",
    "print(history.history['val_auroc'][-1])\n",
    "\n",
    "y_pred = model.predict(test_ent_matrix,\n",
    "                       batch_size = batch_size)\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    pos_label=1,\n",
    ")\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "drop_out_rates = [0.1,0.2,0.3,0.4,0.5]\n",
    "for drop_outs in drop_out_rates:\n",
    "    x_input = Input(shape=num_features)\n",
    "    x_num = Dense(256,activation='relu')(x_input)\n",
    "    x_num = Dropout(drop_outs)(x_num)\n",
    "    x_num = Dense(128,activation='relu')(x_num)\n",
    "    x_num = Dropout(drop_outs)(x_num)\n",
    "    x_num = Dense(64,activation='relu')(x_num)\n",
    "    x_num = Dropout(drop_outs)(x_num)\n",
    "    x_num = Dense(32,activation='relu')(x_num)\n",
    "    x_output = Dense(1,activation='sigmoid')(x_num)\n",
    "    model = Model(inputs=x_input, outputs=x_output)\n",
    "    opt = Adam()\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', auroc])\n",
    "    model.summary()\n",
    "\n",
    "    patience = 50\n",
    "    batch_size = 256\n",
    "    epochs = 1000\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_val_scaled = scaler.transform(x_val)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_auroc', patience=patience)]\n",
    "    history = model.fit(x_train_scaled,y_train,\n",
    "                                 validation_data = (x_val_scaled,y_val),\n",
    "                                 batch_size = batch_size,\n",
    "                                 epochs = epochs,\n",
    "                                 callbacks = callbacks,\n",
    "                                 verbose = 0)\n",
    "\n",
    "    plt.plot(np.arange(len(history.history['val_auroc'])),history.history['val_auroc'],label='validation')\n",
    "    plt.plot(np.arange(len(history.history['val_auroc'])),history.history['auroc'],label='train')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('ROC AUC')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(history.history['val_auroc'][-1])\n",
    "\n",
    "    y_pred = model.predict(x_test_scaled,\n",
    "                           batch_size = batch_size)\n",
    "\n",
    "    fpr, tpr, _ = metrics.roc_curve(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        pos_label=1,\n",
    "    )\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(x_train.flatten()).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(x_train.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(history.history['val_auroc'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = Input(shape=train_ent_matrix.shape[1])\n",
    "x_output = Embedding(input_dim = max_pos_len,\n",
    "                    output_dim = 64)(x_input)\n",
    "x_output = Dense(32,activation='relu')(x_output)\n",
    "x_output = GlobalAveragePooling1D()(x_output)\n",
    "x_output = Dense(1,activation='sigmoid')(x_output)\n",
    "model = Model(inputs=x_input, outputs=x_output)\n",
    "opt = Adam()\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', auroc])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patience = 10\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=patience)]\n",
    "history = model.fit(train_ent_matrix,y_train,\n",
    "                             validation_data = (val_ent_matrix,y_val),\n",
    "                             batch_size = batch_size,\n",
    "                             epochs = epochs,\n",
    "                             callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(history.history['val_auroc'])),history.history['val_auroc'],label='validation')\n",
    "plt.plot(np.arange(len(history.history['val_auroc'])),history.history['auroc'],label='train')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = Input(shape=x_train.shape[1])\n",
    "x_output = Dense(256,activation='relu')(x_input)\n",
    "x_output = Dense(128,activation='relu')(x_output)\n",
    "x_output = Dense(64,activation='relu')(x_output)\n",
    "x_output = Dense(1,activation='sigmoid')(x_output)\n",
    "model = Model(inputs=x_input, outputs=x_output)\n",
    "opt = Adam()\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', auroc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patience = 10\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=patience)]\n",
    "history = model.fit(x_train,y_train,\n",
    "                             validation_data = (x_val,y_val),\n",
    "                             batch_size = batch_size,\n",
    "                             epochs = epochs,\n",
    "                             callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(history.history['val_auroc'])),history.history['val_auroc'],label='validation')\n",
    "plt.plot(np.arange(len(history.history['val_auroc'])),history.history['auroc'],label='train')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train,return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
